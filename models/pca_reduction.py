import os
import numpy as np
import joblib
import warnings
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# âœ… Paths
feature_path = "C:/Users/nniha/flask_env/Scripts/project/models/features.npy"
pca_feature_path = "C:/Users/nniha/flask_env/Scripts/project/models/X_pca.npy"
pca_model_path = "C:/Users/nniha/flask_env/Scripts/project/models/pca_mod.pkl"
pca_variance_path = "C:/Users/nniha/flask_env/Scripts/project/models/pca_variance.npy"

# âœ… Load features safely
if not os.path.exists(feature_path):
    raise FileNotFoundError("âŒ Error: Features file not found!")

X = np.load(feature_path)

# âœ… Handle empty dataset
if X.size == 0 or X.shape[0] == 0:
    raise ValueError("âŒ Error: No features found in the dataset!")

if X.shape[0] < 2:
    raise ValueError("âŒ Error: At least 2 samples are required for PCA!")

# âœ… Reshape if features are in (samples, 1, 2048) format
if len(X.shape) == 3 and X.shape[1] == 1:
    X = X.reshape(X.shape[0], X.shape[2])  # Reshape to (samples, 2048)
elif len(X.shape) != 2:
    raise ValueError(f"âŒ Error: Unexpected feature shape: {X.shape}. Expected (samples, features).")

# âœ… Handle `NaN` or corrupted values
if not np.isfinite(X).all():
    raise ValueError("âŒ Error: Dataset contains NaN or infinite values! Check preprocessing.")

# âœ… Standardize features before PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# âœ… Find optimal `n_components` to preserve 95% variance
pca_full = PCA()
pca_full.fit(X_scaled)  # Fit PCA to compute explained variance
explained_variance = np.cumsum(pca_full.explained_variance_ratio_)

# âœ… Select `n_components` where variance â‰¥ 95%
n_components = np.argmax(explained_variance >= 0.95) + 1
n_components = max(2, min(n_components or min(64, X.shape[1]), 64))

# âœ… Apply PCA with optimal components
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# âœ… Prevent accidental overwriting of PCA files
# âœ… Prevent accidental overwriting of PCA files
for path in [pca_feature_path, pca_model_path, pca_variance_path]:
    backup_path = path + ".backup"
    if os.path.exists(backup_path):
        os.remove(backup_path)  # Delete old backup
    if os.path.exists(path):
        os.rename(path, backup_path)  # Rename safely



print("ğŸ”„ Previous PCA files backed up (if existed). Saving new files...")

# âœ… Save reduced features, PCA model, and explained variance
try:
    np.save(pca_feature_path, X_pca)
    joblib.dump(pca, pca_model_path)
    np.save(pca_variance_path, pca.explained_variance_ratio_)
except Exception as e:
    print(f"âŒ Error saving PCA files: {e}")
    exit(1)
finally:
    print("ğŸ”„ File save operation completed. Check logs if any issues.")

# âœ… Print Summary
print(f"âœ… PCA Features saved at: {pca_feature_path}")
print(f"ğŸ’¾ PCA Model saved at: {pca_model_path}")
print(f"ğŸ“Š Original Features Shape: {X.shape}")
print(f"ğŸ“‰ Reduced PCA Features Shape: {X_pca.shape}")
print(f"ğŸ” Selected Components: {n_components}")
print(f"ğŸ“ˆ Explained Variance Preserved: {explained_variance[n_components-1]:.4f}")
